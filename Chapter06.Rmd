---
title: "回归分析"
author: "梁雪枫"
date: "2014年12月10日"
documentclass: ctexart
output:
  pdf_document:
    fig_caption: yes
    latex_engine: xelatex
    number_sections: yes
    template: !expr rticles::ctex_template()
    toc: yes
classoption: "hyperref`r if (.Platform$OS.type != 'windows') ',nofonts'`"
---

```{r include=FALSE}
library(pander)
library(ggplot2)
library(pspearman)
library(gvlma)
library(car)
library(lmtest)
library(leaps) 
library(plyr) 
library(bootstrap) 
```
因变量是分类的，则为分类分析，因变量是连续的，称为回归分析。回归分析通过建立函数表达式，用一个或者多个自变量的变化解释或预测因变量，常用于描述、探索和检验自变量和因变量之间因果关系，根据自变量的变化预测因变量的取值。通常按照自变量的个数划分为一元回归和多元回归。按照函数表达式的形式，分为线性回归和非线性回归。

##一元线性回归
假设有两个变量X和Y，X为自变量，Y为因变量。则一元线性回归模型的基本结构形式为$$Y=\beta _{0}+\beta _{1}X+\varepsilon $$。$\varepsilon$是误差项，表示未知或不易测量的随机因素对因变量影响的总和。$\beta _{0}$为回归的常数，$\beta _{1}$为回归系数，它表示增加和减少一个单位时，Y的平均变化量。线性回归就是根据已经观测到的样本数据，应用最小二乘法获得对$\beta _{0}$和$\beta _{1}$的估计，进而得到回归方程。由于参数估计时并不知道是否存在线性关系，回归方程在应用前需要完成对回归方程的检验，即对回归模型的系数是否为零进行检验。常用t检验、F检验和相关系数检验评价回归方程的回归系数是否为0。
线性回归应用有四个前提条件：线性、独立、正态和方差齐性。线性指自变量和因变量在散点图大致呈直线趋势。独立性值观察值之间应相互独立。正态性指残差应符合正态分布。方差齐性指在自变量取值范围内，对于自变量的取值，因变量都有相同的方差。

例1 某医生分别采用盐析法和结合法测定正常皮肤中胶原蛋白的含量。盐析法只能部分提纯，结合法较为复杂单精确。该医生欲寻求找盐析法和结合法之间的关系，以便通过盐析法预测结合法的测定值。

编号   盐析法   结合法
---    ---      ---
1      6.8      546
2      7.8      553
3      8.7      562
4      8.7      563
5      8.9      570
6      9.5      575
7      10.1     581
8      10.2     605
9      10.3     607
10     10.4     621
11     11.1     624
12     12.4     626
13     13.3     632
14     13.1     640
15     13.2     656
 

解 做散点图，观察是否存在线性关系。由于线性回归的条件（线性、正态性、方差齐性和独立性）是通过残差来完成的，可先建立回归方程，然后通过回归诊断来完成线性回归条件的检验。
线性条件可通过散点图直接观察。
```{r}
y <- c(546,553,562,563,570,575,581,605,607,621,624,626,632,640,656)
x <- c(6.8,7.8,8.7,8.7,8.9,9.5,10.1,10.2,10.3,10.4,11.1,12.4,13.3,13.1,13.2)
df <- as.data.frame(cbind(x,y))

#ggplot(df,aes(x,y))+geom_point()+stat_smooth(method = "lm")
plot(x,y)
```
通过散点图，可以发现二者近似直线关系，符合线性条件。

对数据进行线性回归分析。
```{r}
model <- lm(y~x) #~左边为响应变量，右边为各个预测变量，预测变量之间用+符号分隔
summary(model)
```
结果中Coefficients是参数估计的结果。结果显示，截距项和回归系统均有统计学意义。模型简单评价$R^{2}$为0.9017，校正决定系数$R_{adj}^{2}$为0.9841。决定系数越大表明自变量对因变量的解释程度越高。F检验检验所有的预测变量预测响应变量是否都在某个几率水平之上，结果表明(F=119.2,P=6.426e-08),方程总体有统计学意义。残差的标准误则可认为模型用自变量预测因变量的平均误差。
所建立的方程为
Y=426.625+16.580×X
对估计值做出区间估计

```{r}
confint(model)
```

```{r}
plot(df$x,df$y)
abline(model)
```
###模型评价

####回归诊断 
主要包括三方面：（1）误差项是否满足独立性、等方差性和正态性，选择模型是否合适。（2）是否存在异常样本，回归分析的结果是否对某些样本依赖过重，即回归模型是否具备稳定性。（3）自变量之间是否存在高度相关，即是否有多重共线性问题存在。

```{r}
par(mfrow=c(2,2))
plot(model) 
par(mfrow=c(1,1))
```
标准方法
正态性 当预测变量值固定时，因变量成正态颁，则残差图也应是一个均值为0的正态颁。正态Q-Q图是在正态颁对应的值上，标准化残差的概率图，若满足正态假设，则图上的点应该落在45度角的直线上，若不是，则违反了正态性假设。第二幅Normal QQ-plot图中数据点分布趋于一条直线, 说明残差是服从正态分布的。

独立性 无法从图中分辨因变量值是否相互独立，只能从收集的数据中验证。本例中适用结合法进行测量时，无理由相信一个测量结果会影响另外一次的测量。

线性 若因变量与自变量线性相关，则残差值与预测（拟合）值就没有系统关联，若存在关系，则说明可能城要对回归模型进行调整。第一幅图Residual vs fitted为拟合值y对残差的图形, 可以看出,数据点都基本均匀地分布在直线y=0的两侧, 无明显趋势，满足线性假设。

方差齐性 若满足不变方差假设，则在第三幅图位置尺度图（Scale-Location Graph）中，水平线周围的点应随机分布，Scale-Location 图显示了标准化残差(standardized residuals)的平方根的分布情况，最高点为残差最大值点。第三副图显示基本符合方差齐性的要求。

第四幅图（Residuals vs Leverage）提供了单个观测点的信息，从图中可以鉴别离群点、高高杆值点和强影响点。

改进方法

正态性 通过对残差正态性检验予以证实。
```{r}
model <- lm(y~x)
shapiro.test(residuals(model))
```
正态性检验结果表明W值为`r shapiro.test(residuals(model))$statistic`，P值为`r shapiro.test(residuals(model))$p.value`,残差符合正态分布。

独立性 判断因变量（或残差）最好的方法时依据收集数据的方式的先验知识。lmtest包提供了dwtest检验函数，car包提供了Durbin-Watson检验的函数，都能够检验误差序列的相关性。
```{r}
dwtest(model)
#durbinWatsonTest(model)
```
本例结果P值比较显著，但根据先验知识，并不能否定因变量的独立性。

线性 可通过成分残差图(component plus residual plot)即偏残差图(partial residual plot)，判断因变量与自变量之间是否呈非线性关系，也可以看是否不同于已设定线性模型的系统偏差，图形可用car包中crPlots()函数绘制。图形存在非线性，则说明可能对预测变量的函数形式建模不够充分.
```{r}
crPlots(model) 
```
图形呈现线性，建模比较充分。

方差齐性 通过以因变量为x轴，学生化残差为y轴做残差图，进行判断。
```{r}
plot(predict(model),rstudent(model))
```
所有的学生化残差均在$\pm 2$在范围内波动，没有明显的上升或下降趋势，可以认为符合方差齐性。还通过自变量与残差绝对值的等级相关检验来判断。

```{r}
spearman.test(x,abs(residuals(model))) #R中pspearman包中的spearman.
#test函数可以完成斯皮尔曼等级相关检验

#cor.test(x,abs(residuals(model)),method="spearman") #或者用cor.test()
```
自变量与残差绝对值的等级相关系数为`r cor.test(x,abs(residuals(model)),method="spearman")$p.value`,P值大于0.05，无统计学意义，可以认为残差方差齐性。

car包提供了两个有用的函数，可判断误差方差是否恒定，ncvTest()函数生成一个计分检验，零假设为误差方差不变,备择假设为误差方差随着拟合值水平的变化而变化。若检验显著，择说明存在异方差性。spreadLevelPlot()函数创建一个添加了最佳拟合曲线的散点图，展示标准化残差绝对值与拟合值的关系。
```{r}
ncvTest(model)  
spreadLevelPlot(model) 
```
计分检验结果不显著，说明满足方差齐性的假设。通过水平分布图，可以看到其中的点在水平的最佳拟合曲线周围呈水平随机分布。若违反了该假设，将会呈现一个非水平的曲线。


线性模型假设的综合验证
```{r}
gvlma(model)
```
Global Stat给模型假设提供了一个单独的综合检验（通过/不通过），本例中，可以看到数据满足线性回归模型所有的统计假设（P=0.7071）。同时还对偏度(Skewness)、峰度(Kurtosis)、连接函数(Link function)和异方差性(Heteroscedasticity)进行了评价。

####影响分析
是探查对估计有异常影响的数据，如果一个样本不服从某个模型，其余数据服从这个模型，则称该样本点为强影响点。影响分析就是区分这样的样本数据。

#####离群点
指那些模型预测效果不佳的观测点，通常有很大的、或正或负的残差，正残差说明模型低估了响应值，负残差说明高佑了响应值。

```{r}
outlierTest(model) #Bonferroni离群点检验
qqPlot(model,labels=row.names(df),id.method = "identify",simulate=T,main="QQPlot") #car包
```
outlierTest（）函数是根据单个最大（或正或负）残差值的显著性来判断是否有离群点，若不显著，则说明数据集中没有离群点，若显著，则必须删除该离群点，然后再检验是否还有其他离群点存在。qqPlot图中落在置信区间带外的点可被认为时离群点。本例中未发现有离群点。

#####高杠杆值点
是与其他预测变量有关的离群点，即它们是由许多异常的预测变量组合起来的，与响应变量值没有关系。
高杠杆值的观测点可通过帽子矩阵的值（hat statistic）判断。对于一个给定的数据集，帽子均值为p/n，其中p是模型估计的参数数目（包含截距项），n是样本量。一般来说，若观测点的帽子值大于帽子均值的2或3倍，则可认定为高杠杆值点。
```{r}
 hat.plot<-function(fit){  
  p<-length(coefficients(fit))  
  n<-length(fitted(fit))  
  plot(hatvalues(fit),main="Index Plot of Hat Values")  
  abline(h=c(2,3)*p/n,col="red",lty=2)  
  identify(1:n,hatvalues(fit),names(hatvalues(fit)))  
}  
hat.plot(model) 
```
此图中可以看到1号点是高杠杆值点。

#####强影响点
强影响点，即对模型参数估计值影响有些比例失衡的点。例如，当移除 模型的一个观测点时模型会发生巨大的改变，那么需要检测一下数据中是否存在强影响点。Cook距离，或称为D统计量。Cook's D值大于4/(n-k-1)，则表明它是强影响点，其中n为样本量大小，k是预测变量数目（有助于鉴别强影响点，但并不提供关于这些点如何影响模型的信息）。
```{r}
plot(model,which=4)
```
Cook距离(Cook’s distance)图显示了对回归的影响点。根据Cook距离，13号点可能是个强影响点。

帽子统计量、DFFITS准测、Cook统计量和COVRATIO准则在R软件可分别通过hatvalues(),dffits(),cooks.distance()和covration()函数计算。influence.measures()可对一次获得这四个统计量的结果。
影响分析综合分析 
```{r}
influencePlot(model) 
#car包中的influencePlot（）函数，可将离群点、
#杠杆点和强影响点的信息整合到一幅图形中
influence.measures(model)
```
纵坐标超过2或小于-2的州可被认为是离群点，水平轴超过0.2或0.3的州有高杠杆值（通常为预测值的组合）。圆圈大小与影响成比例，圆圈很大的点可能是对模型估计造成的不成比例影响的强影响点。influence.measures()的inf用×标注异常值。

###共线性，条件数 
本例只有一个自变量，不涉及。

###预测新值及其置信区间
把预测变量数据保存为一个数据框，调用predict函数，将数据框做为参数。
```{r}
preds <- data.frame(x=14)
#默认0.95的置信水平，可通过level改变
predict(model,newdata = preds,interval = "prediction") 
```
###改进措施
```{r}
model2 <- lm(y~x,subset=-1)
gvlma(model2)
influence.measures(model2)
```
提示2号点也是是个异常值。
```{r}
model3 <- lm(y~x,subset=c(-1,-2))
gvlma(model3)
influence.measures(model3)
```
删除1、2观测值后，模型的影响分析的结果变得更好。但应该对删除观测点的方法谨慎，因为收集数据的异常点可能是最有意义东西，除非确定数据点时记录错误或者没有相关遵守规程。

##多元线性回归
例２　某项“冠状动脉缓慢血流现象”的影响因素的研究，以前降支、回旋支、右冠状动脉三支血管的平均TIMI帧基数(MTFC)表示，调查的影响因素有年龄(AGE,岁)、收缩压(SBP,mmHg)、舒张压(DBP,mmHg)、白细胞(WBC,$10^{2}$/L),寻找影响MTFC变化的因素。

age sbp dbp wbc   mtfc
--- --- --- ---   ---
43  110 50	6.19	33.67
63	105	60	6.03	26.67
59	100	60	5.28	23
78	100	60	6.52	26
67	100	60	7.31	28
65	119	61	5.67	30.33
66	120	64	5.11	27
73	130	88	6.40  47
53	113	68	4.41	27.67
76	120	70	4.20	37.33
76	136	70	5.38	35.67
76	130	70	4.94	31.33
68	126	70	4.56	32.33
61	136	70	5.42	30.67
78	124	70	5.75	37.67
80	110	70	4.68	36
74	140	70	8.67	41
75	130	70	6.62	41.67
66	130	70	6.86	22
55	114	70	7.52	23.33
71	120	70	4.94	25.67
62	130	70	4.59	25
69	130	70	4.26	27
45	110	70	10.21	29
79	120	70	6.46	30.33
58	110	70	4.70	27
65	100	70	6.06	28
44	119	70	5.55	22.33
53	110	70	14.0	29.33
62	130	72	7.29	43
62	118	72	3.97	27.33
53	122	74	3.97	18.33
71	130	75	3.78	31
54	116	75	4.35	22.33
64	120	76	6.59	30
71	140	78	5.70	35.67
50	121	78	5.27	40.33
51	138	80	5.65	34.67
73	130	80	7.45	35.33
64	138	80	6.58	33.67
40	130	80	7.51	35.33
72	120	80	4.42	34
51	100	80	7.85	21
49	120	89	5.14	20.67
63	150	90	8.18	42.67
56	130	90	5.23	30.67
69	160	90	7.10	39
78	130	90	6.03	29
78	120	90	4.52	30.67
61	150	92	7.52	40
76	142	92	4.66	38
51	140	100	5.70	28.33
51	140	100	6.71	42.67
57	160	100	6.14	41
63	190	100	5.25	46
69	150	80	6.33	22.67

```{r warning=FALSE}
records <- read.table("example1")
ex <- rename(records, c("V1"="age","V2"="sbp","V3"="dbp","V4"="wbc","V5"="mtfc"))
attach(ex)
#scatterplotMatrix()函数默认在非对角线区域绘制变量间的散点图，并添加平滑（loess）和线性拟合区间
scatterplotMatrix(ex,spread=F,lty.smooth=2,main="scatter plot matrix")
```
绘制因变量与自变量的散点图矩阵显示mtfc和wbc线性关系不是很好。首先做单因素的线性回归，尽管有时候单因素的分析不是必须的，其结果也不一定可靠，但有助于初步探索自变量和因变量之间的关系。
```{r}
summary(lm(mtfc~age))
summary(lm(mtfc~sbp))
summary(lm(mtfc~dbp))
summary(lm(mtfc~wbc))
summary(lm(mtfc~age+sbp+dbp+wbc))
```
分析结果表明，单因素分析中dbp有统计学，而多因素分析中却没有统计学意义。分析自变量的相关系数
```{r}
cor(ex[1:4])
cor.test(dbp,sbp)
```
相关分析结果表明sbp和dbp有明显的正相关作用，说明单因素分析中dbp对因变量的作用同时包含了部分sbp的正向作用。在删除dbp变量，继续建模。
```{r}
summary(lm(mtfc~age+sbp+wbc))
```
结果显示三个因素均有统计学意义，F检验也通过了，但决定系数较低，说明自变量对因变量的解释程度较低。查看wbc变量估计结果，其标准误为0.44远高于age和sbp变量，计算这三个变量的变异系数。
```{r}
cv <- function(x){
  return(100*sd(x)/mean(x))
}

cv(age)
cv(sbp)
cv(wbc)
```
wbc变量的变异系数远高于其他连个变量，为减少wbc变量的变异，对其进行对数变换后重新建模
```{r}
fit <- lm(mtfc~age+sbp+log10(wbc))
summary(fit)
gvlma(fit)
influence.measures(fit)
```
所建立的模型通过回归诊断，影响分析存在异常点，但没有理由怀疑是异常点所以予以保留。


###多重共线性
指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。一般来说，由于数据的限制使得模型设计不当，导致设计矩阵中解释变量间存在普遍的相关关系目前常用的多重共线性诊断方法
1.自变量的相关系数矩阵R诊断法：研究变量的两两相关分析，如果自变量间的二元相关系数值很大，则认为存在多重共线性。但无确定的标准判断相关系数的大小与共线性的关系。有时，相关系数值不大，也不能排除多重共线性的可能。

2.方差膨胀因子（the variance inflation factor，VIF)诊断法：方差膨胀因子表达式为：$VIF_{i}=1/（1-R^{2}_{i})$。其中Ri为自变量$x_{i}$对其余自变量作回归分析的复相关系数。当$VIF_{i}$很大时，表明自变量间存在多重共线性。该诊断方法也存在临界值不易确定的问题，在应用时须慎重。

3.容忍值（Tolerance，简记为Tol）法：容忍值实际上是VIF的倒数，即Tol＝1/VIF。其取值在0～1之间，Tol越接近1，说明自变量间的共线性越弱。在应用时一般先预先指定一个Tol值，容忍值小于指定值的变量不能进入方程，从而保证进入方程的变量的相关系数矩阵为非奇异阵，计算结果具有稳定性。但是，有的自变量即使通过了容忍性检验进入方程，仍可导致结果的不稳定。

4.多元决定系数值诊断法：假定多元回归模型p个自变量，其多元决定系数为R2y（X1，X2,…，Xp）。分别构成不含其中某个自变量（$x_{i}$,i=1,2,…，p）的p个回归模型，并应用最小二乘法准则拟合回归方程，求出它们各自的决定系数$R^{2}_{i}（i=1,2,…，p）$。如果其中最大的一个R2k与R2Y很接近，就表明该自变量在模型中对多元决定系数的影响不大，说明该变量对Y总变异的解释能力可由其他自变量代替。它很有可能是其他自变量的线性组合。因此，该自变量进入模型后就有可能引起多重共线性问题。该方法也存在临界值和主观判断问题。

5.条件数与特征分析法：在自变量的观测值构成的设计矩阵X中，求出变量相关系数R的特征值，如果某个特征值很小（如小于0．05 ），或所有特征值的倒数之和为自变量数目的5倍以上，表明自变量间存在多重共线性关系。利用主成分分析，如果X′X的特征值RK小于0．05时，RK所对应的主成分FK可近似为零，表明自变量间存在K个多重共线性关系。 

从实际经验的角度,一般若条件数<100,则认为多重共线性的程度很小,若100<=条件数<=1000,则认为存在中等程度的多重共线性,若条件数>1000,则认为存在严重的多重共线性。kappa大于1000，或vif大于10说明存在多重共线性。在R中判断多重共线性的命令为kappa（条件数），vif（方差膨胀因子）

```{r}
kappa(cor(ex))
vif(fit) #car包
sqrt(vif(fit)) > 2
```
一般来说kappa大于1000，或vif大于10说明存在多重共线性,vif开平方是否大于2，若大于2，则存在多重共线性问题。本例中未发现存在多重共线性。


###模型比较
AIC(Akaike Information Criterion,赤池信息准则)也可以用来比较模型,它考虑了模型的统计拟合度以及用来拟合的参数数目。AIC值越小的模型要优先选择,它说明模型用较少的参数获得了足够的拟合度。

```{r}
fit2 <- lm(mtfc~age+sbp+wbc)
AIC(fit,fit2)
```
选择AIC值较小的模型lm(mtfc~age+sbp+log10(wbc))。

尽管log10(wbc)回归系数最高，但并不代表log10(wbc)对mtfc的影响最大，因为三个变量的单位不同。对于不同的单位，如果要衡量对因变量大小的影响，需采用标准化回归系数。
```{r}
lm.beta <- function(MOD) 
{
  b <- summary(MOD)$coef[-1, 1]
  sx <- sapply(MOD$model[-1], sd)
  sy <- sapply(MOD$model[1], sd)
  beta <- b * sx/sy
  return(beta)
}
lm.beta(fit)
detach(ex)
```
尽管通过模型比较，获得了lm(mtfc~age+sbp+log10(wbc))模型，模型的回归诊断也能通过，但从决定系数来看自变量对因变量的解释程度并不高。

##逐步回归
实际中，影响因变量的自变量较多，对如何从自变量中选择若干个，得到最佳的回归方程，在不同的准则下有不同的方法来获得最佳回归方程。对于一个包含n个自变量的的回归问题，全部的回归模型将有$2^{n}-1$个。常用的逐步方法有“向前法”，“向后法”，“逐步法”和“最优子集法”。在R中，通过step()函数的direction = c("both", "backward", "forward")选项分别完成“逐步回归法”、“向后法”和“向前法”。leaps包可以完成全子集回归法,leaps()函数以$C_{p}$准则（默认）、校正$R^{2}$和$R^{2}$来选择全局最优模型。

例3 有5个自变量x1～x5和1个因变量，请用逐步回归完成自变量的筛选。
```{r}
x1<- c(7,1,11,11,7,11,3,1,2,21,1,11,10)
x2<- c(26,29,56,31,52,55,71,31,54,47,40,66,68)
x3<- c(6,15,8,8,6,9,17,22,18,4,23,9,8)
x4<- c(60,52,20,47,33,22,6,44,22,26,34,12,12)
y<- c(78.5,74.3,104.3,87.6,95.9,109.2,102.7,72.5,93.1,115.9,83.8,113.3,109.4)
df <- as.data.frame(cbind(x1,x2,x3,x4,y))

leapmodels <- leaps(x = cbind(x1,x2,x3,x4),y = y)
plot(leapmodels$size, leapmodels$Cp)
abline(0,1)
cbind(leapmodels$size,leapmodels$which, leapmodels$Cp)
```
选择$C_{p}$统计量最小的变量集合，本例中$C_{p}$统计量最小值所对应的变量集合为x1和x2。结果中1为选中，0为未选中。
```{r}
subsets <- regsubsets(y~x1+x2+x3+x4,data=df)
summary(subsets)
plot(subsets,scale="adjr2") #基于调整R平方，不同子集大小的最佳模型
```
图的顶部的图形便是最适合的模型，校正R平方值0.98最高，x1和x2两预测变量是最佳模型。也可以通过$C_{p}$统计量完成变量的选择。
```{r}
sbs<- regsubsets(y~x1+x2+x3+x4,data=df)
subsets(sbs,legend=FALSE,statistic="cp",main="cp plot for all subsets regression")
abline(1,1,lty=2,col="red") #画截距项和斜率均为1的直线
summary(sbs)
```
$C_{p}$图越好的模型离截距项和斜率均为1的直线越近，x1-x2、x1-x2-x4和x1-x2-x3-x4均与直线比较接近，这三个模型根据$C_{p}$统计量结果类似。

```{r}
fit <- lm(y~x1+x2+x3+x4,data=df)
fit.step <- step(fit)  #基于AIC
summary(fit.step)
```
step()函数通过AIC信息准则，删除了x3变量后AIC值最小24.97，得到y ~ x1 + x2 + x4。回归系数的显著性检验水平有较大提升，但x2和x4的系数检验仍不理想。
```{r}
drop1(fit.step)
```
去掉x4后，AIC值会上升到25.42，残差的平方和会上升到9.93，是上升最少的。去掉x4后
```{r}
lm.opt<-lm(y ~ x1+x2, data=df)
summary(lm.opt)
gvlma(lm.opt)
influence.measures(lm.opt)
sqrt(vif(lm.opt)) > 2
```
检验结果显著，回归诊断和多重共线性均通过，影响分析中只发现有一个异常点。因为全子集回归考虑了更多模型，全子集回归要优于逐步回归，但当自变量个数较多时，全子集回归较慢。一般来所，变量的自动筛选应建立在背景知识理解基础上进行，防止出现拟合效果好，但没有实际意义的模型。

##交叉验证
就是按一定比例将原始数据按照拆分成训练集和测试集，现在训练集上获取回归方程，然后在测试集上做预测。由于测试集不涉及模型参数的选择，该样本可获得比新数据获得更为精确的估计。$k$重交叉验证中，样本被分为$k$个子样本，轮流将$k-1$个子样本作为训练集，另外1个样本作为测试集。通过获得$k$个预测方程，记录$k$个测试集的预测结果，然后求其平均值。

```{r}
shrinkage<-function(fit,k=5){  
  require(bootstrap)  
  theta.fit<-function(x,y){lsfit(x,y)}  
  theta.predict<-function(fit,x){cbind(1,x)%*%fit$coef}  
  x<-fit$model[,2:ncol(fit$model)]  
  y<-fit$model[,1]  
  results<-crossval(x,y,theta.fit,theta.predict,ngroup=k)  
  r2<-cor(y,fit$fitted.values)^2  
  r2cv<-cor(y,results$cv.fit)^2  
  cat("Original R-square=",r2,"\n")  
  cat(k,"Fold Cross-Validated R-square=",r2cv,"\n")  
  cat("Change=",r2-r2cv,"\n")  
}  

fit <- lm(y~x1+x2,data=df)
shrinkage(fit)
```
获得原始R平方为0.9786,交叉验证后的R平方为0.9962（基于BootStrap方法，每次运行结果会有不同）。R平方减少得越少，预测越精准。

##相对重要性
评价自变量相对重要性，最简单的方法为比较标准化的回归系数，它表示当其他自变量不变时，该自变量变化1个单位引起的因变量的变化。前面通过lm.beta()函数获得了标准化的回归系数。基于相对权重的重要性测量，是对所有可能自模型添加一个自变量引起的R平方平均增加量的一个近似值，比标准化回归系数更为直观。

```{r}
relweights<-function(fit,...){  
  R<-cor(fit$model)  
  nvar<-ncol(R)  
  rxx<-R[2:nvar,2:nvar]  
  rxy<-R[2:nvar,1]  
  svd<-eigen(rxx)  
  evec<-svd$vectors  
  ev<-svd$values  
  delta<-diag(sqrt(ev))  
  lambda<-evec%*%delta%*%t(evec)  
  lambdasq<-lambda^2  
  beta<-solve(lambda)%*%rxy  
  rsquare<-colSums(beta^2)  
  rawwgt<-lambdasq%*%beta^2  
  import<-(rawwgt/rsquare)*100  
  lbls<-names(fit$model[2:nvar])  
  rownames(import)<-lbls  
  colnames(import)<-"Weight"  
  barplot(t(import),names.arg=lbls, 
          ylab="% of R-Square",  
          xlab="Predictor Variables",  
          main="Relative Importance of Predictor Variables",  
          sub=paste("R-Square=",round(rsquare,digits=3)),...)  
  return(import)  
}  
fit <- lm(y~x1+x2,data=df)
relweights(fit,col="lightgrey")  
```

可以看到x2解释了56.7%的R平方，x1解释了43.2的平方，x2相比x1更为重要。










